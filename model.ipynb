{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece datasets"
      ],
      "metadata": {
        "id": "5e_0BgWs_5mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"emotion\")\n",
        "ds"
      ],
      "metadata": {
        "id": "nvczl2vzg-Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'microsoft/deberta-v3-small'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok_func(x): return tokenizer(x[\"text\"])\n",
        "\n",
        "tok_ds = ds.map(tok_func, batched=True)\n",
        "tok_ds"
      ],
      "metadata": {
        "id": "HHO1weN7_HUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "bs = 128\n",
        "epochs = 4\n",
        "lr = 1e-4\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True, \n",
        "                         evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, \n",
        "                         num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
        "\n",
        "trainer = Trainer(model, args, train_dataset=tok_ds['train'], eval_dataset=tok_ds['validation'], \n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "W727G03S58ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "label_map={\n",
        "    'LABEL_0':'üôÅ',\n",
        "    'LABEL_1':'üòÉ',\n",
        "    'LABEL_2':'ü•∞',\n",
        "    'LABEL_3':'üò†',\n",
        "    'LABEL_4':'üò¨',\n",
        "    'LABEL_5':'üò≥'\n",
        "  }\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "text = 'my very first nlp model'\n",
        "emotion = label_map[pipe(text)[0]['label']]\n",
        "print(f'{text} {emotion}')"
      ],
      "metadata": {
        "id": "hA1Hdmng7cHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}